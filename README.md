# ğŸ§  GenAI Flask App  
A lightweight yet productionâ€‘ready AI web application built with **Flask**, **IBM Watsonx**, and **LangChain**, supporting multiple LLMs including **Llama**, **Granite**, and **Mistral**.

This project demonstrates fullâ€‘stack AI engineering: model orchestration, prompt templating, JSONâ€‘structured outputs, and cloud deployment.

---

## ğŸš€ Overview

GenAI Flask App is a complete endâ€‘toâ€‘end AI assistant featuring:

- ğŸŒ A clean web interface (HTML + JavaScript)
- ğŸ§© A Flask backend with REST API endpoints
- ğŸ¤– Multiâ€‘model support (Llama / Granite / Mistral)
- ğŸ§  LangChain prompt templates + JSON output parsing
- ğŸ” Secure environment variable handling (no hardâ€‘coded keys)
- â˜ï¸ Deployment on Render with public access

Itâ€™s designed as a practical, realâ€‘world AI application suitable for learning, showcasing, or extending into a larger product.

---

## ğŸ› ï¸ Tech Stack

### **Backend**
- Python 3.10+
- Flask
- LangChain
- IBM Watsonx AI SDK

### **Frontend**
- HTML / CSS / JavaScript
- Fetch API

### **Deployment**
- Render Web Service
- GitHub for version control
- Environment variables for secrets

---

## ğŸ“ Project Structure

```
genai_flask_app/
â”‚â”€â”€ app.py                # Flask backend API
â”‚â”€â”€ model.py              # Model logic (Llama/Granite/Mistral)
â”‚â”€â”€ templates/
â”‚     â””â”€â”€ index.html      # Frontend UI
â”‚â”€â”€ static/
â”‚     â””â”€â”€ script.js       # Frontend logic
â”‚â”€â”€ requirements.txt      # Dependencies
â”‚â”€â”€ README.md             # Project documentation
```

---

## ğŸ”§ Running Locally

### 1. Clone the repository
```bash
git clone https://github.com/Buffonite/genai_flask_app
cd genai_flask_app
```

### 2. Install dependencies
```bash
pip install -r requirements.txt
```

### 3. Set environment variables
```
WATSONX_API_KEY=your_api_key
WATSONX_PROJECT_ID=your_project_id
```

### 4. Start the server
```bash
python app.py
```

Visit:

```
http://localhost:5000
```

---

## â˜ï¸ Deploying to Render

1. Push the project to GitHub  
2. Create a new **Render Web Service**  
3. Configure:

**Build Command**
```
pip install -r requirements.txt
```

**Start Command**
```
python app.py
```

4. Add environment variables:

```
WATSONX_API_KEY=xxxx
WATSONX_PROJECT_ID=xxxx
```

5. Deploy and access your public URL.

---

## ğŸ¤– Model Selection

The frontend sends:

```json
{
  "model": "llama",
  "user_message": "Hello"
}
```

The backend routes the request to the selected model and returns a unified response:

```json
{
  "response": "Model output here",
  "duration": 0.53
}
```

Supported models:

- `llama`
- `granite`
- `mistral`

---

## ğŸ§  Future Enhancements

- Multiâ€‘turn conversation memory  
- Chat history storage  
- User authentication  
- Token usage tracking  
- Improved UI/UX  
- Custom domain  
- Database integration  
- Parallel model inference  

---

## ğŸ“œ License

MIT License

---

## âœ¨ Author

**Barry**  
A handsâ€‘on AI developer passionate about building real, deployable AI applications.
